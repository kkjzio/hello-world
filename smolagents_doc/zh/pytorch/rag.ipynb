{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkjzio/hello-world/blob/master/smolagents_doc/zh/pytorch/rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C99fxKkTZTTB",
        "outputId": "cc07caf0-b4b3-4951-f0bb-eeefad1eddb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting smolagents\n",
            "  Downloading smolagents-1.21.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.31.2 in /usr/local/lib/python3.12/dist-packages (from smolagents) (0.34.4)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from smolagents) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from smolagents) (13.9.4)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from smolagents) (3.1.6)\n",
            "Requirement already satisfied: pillow>=10.0.1 in /usr/local/lib/python3.12/dist-packages (from smolagents) (11.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from smolagents) (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents) (1.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.4->smolagents) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n",
            "Downloading smolagents-1.21.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: smolagents\n",
            "Successfully installed smolagents-1.21.2\n"
          ]
        }
      ],
      "source": [
        "# Installation\n",
        "! pip install smolagents\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/smolagents.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPUhG-KZTTF"
      },
      "source": [
        "# Agentic RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpqneNd2ZTTH"
      },
      "source": [
        "Retrieval-Augmented-Generation (RAG) 是“使用大语言模型（LLM）来回答用户查询，但基于从知识库中检索的信息”。它比使用普通或微调的 LLM 具有许多优势：举几个例子，它允许将答案基于真实事实并减少虚构；它允许提供 LLM 领域特定的知识；并允许对知识库中的信息访问进行精细控制。\n",
        "\n",
        "但是，普通的 RAG 存在一些局限性，以下两点尤为突出：\n",
        "\n",
        "- 它只执行一次检索步骤：如果结果不好，生成的内容也会不好。\n",
        "- 语义相似性是以用户查询为参考计算的，这可能不是最优的：例如，用户查询通常是一个问题，而包含真实答案的文档通常是肯定语态，因此其相似性得分会比其他以疑问形式呈现的源文档低，从而导致错失相关信息的风险。\n",
        "\n",
        "我们可以通过制作一个 RAG  agent来缓解这些问题：非常简单，一个配备了检索工具的agent！这个 agent 将\n",
        "会：✅ 自己构建查询和检索，✅ 如果需要的话会重新检索。\n",
        "\n",
        "因此，它将比普通 RAG 更智能，因为它可以自己构建查询，而不是直接使用用户查询作为参考。这样，它可以更\n",
        "接近目标文档，从而提高检索的准确性， [HyDE](https://huggingface.co/papers/2212.10496)。此 agent 可以\n",
        "使用生成的片段，并在需要时重新检索，就像 [Self-Query](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery/)。\n",
        "\n",
        "我们现在开始构建这个系统. 🛠️\n",
        "\n",
        "运行以下代码以安装所需的依赖包：\n",
        "```bash\n",
        "!pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q\n",
        "```\n",
        "\n",
        "你需要一个有效的 token 作为环境变量 `HF_TOKEN` 来调用 Inference Providers。我们使用 python-dotenv 来加载它。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q"
      ],
      "metadata": {
        "id": "lc1sMNVkZqL_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca7sJpHvZTTH",
        "outputId": "89b799c3-9c7f-47ff-e2b4-4f693255942f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ril9ve6aZTTH"
      },
      "source": [
        "我们首先加载一个知识库以在其上执行 RAG：此数据集是许多 Hugging Face 库的文档页面的汇编，存储为 markdown 格式。我们将仅保留 `transformers` 库的文档。然后通过处理数据集并将其存储到向量数据库中，为检索器准备知识库。我们将使用 [LangChain](https://python.langchain.com/docs/introduction/) 来利用其出色的向量数据库工具。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1PqfMqXEZTTI"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "docs_processed = text_splitter.split_documents(source_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9Pth9s0ZTTI"
      },
      "source": [
        "现在文档已准备好。我们来一起构建我们的 agent RAG 系统！\n",
        "👉 我们只需要一个 RetrieverTool，我们的 agent 可以利用它从知识库中检索信息。\n",
        "\n",
        "由于我们需要将 vectordb 添加为工具的属性，我们不能简单地使用带有 `@tool` 装饰器的简单工具构造函数：因此我们将遵循 [tools 教程](https://huggingface.co/docs/smolagents/main/zh/examples/../tutorials/tools) 中突出显示的高级设置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JvYDQK68ZTTI"
      },
      "outputs": [],
      "source": [
        "from smolagents import Tool\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.retriever = BM25Retriever.from_documents(\n",
        "            docs, k=10\n",
        "        )\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        docs = self.retriever.invoke(\n",
        "            query,\n",
        "        )\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "retriever_tool = RetrieverTool(docs_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERa3z-CVZTTI"
      },
      "source": [
        "BM25 检索方法是一个经典的检索方法，因为它的设置速度非常快。为了提高检索准确性，你可以使用语义搜索，使用文档的向量表示替换 BM25：因此你可以前往 [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) 选择一个好的嵌入模型。\n",
        "\n",
        "现在我们已经创建了一个可以从知识库中检索信息的工具，现在我们可以很容易地创建一个利用这个\n",
        "`retriever_tool` 的 agent！此 agent 将使用如下参数初始化：\n",
        "- `tools`：代理将能够调用的工具列表。\n",
        "- `model`：为代理提供动力的 LLM。\n",
        "\n",
        "我们的 `model` 必须是一个可调用对象，它接受一个消息的 list 作为输入，并返回文本。它还需要接受一个 stop_sequences 参数，指示何时停止生成。为了方便起见，我们直接使用包中提供的 `HfEngine` 类来获取调用 Hugging Face 的 Inference API 的 LLM 引擎。\n",
        "\n",
        "接着，我们将使用 [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/docs/smolagents/main/zh/examples/meta-llama/Llama-3.3-70B-Instruct) 作为 llm 引\n",
        "擎，因为：\n",
        "- 它有一个长 128k 上下文，这对处理长源文档很有用。\n",
        "- 它在 HF 的 Inference API 上始终免费提供！\n",
        "\n",
        "_Note:_ 此 Inference API 托管基于各种标准的模型，部署的模型可能会在没有事先通知的情况下进行更新或替换。了解更多信息，请点击[这里](https://huggingface.co/docs/api-inference/supported-models)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "08Oh_WmlZTTI"
      },
      "outputs": [],
      "source": [
        "from smolagents import InferenceClientModel, CodeAgent\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool], model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\"), max_steps=4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8TW5xVZZTTJ"
      },
      "source": [
        "当我们初始化 CodeAgent 时，它已经自动获得了一个默认的系统提示，告诉 LLM 引擎按步骤处理并生成工具调用作为代码片段，但你可以根据需要替换此提示模板。接着，当其 `.run()` 方法被调用时，代理将负责调用 LLM 引擎，并在循环中执行工具调用，直到工具 `final_answer` 被调用，而其参数为最终答案。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c-L_4tBqZTTJ",
        "outputId": "bf186d3d-a29a-4cb3-8ed6-59bccf117a0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFor a transformers model training, which is slower, the forward or the backward pass?\u001b[0m                           \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - meta-llama/Llama-3.3-70B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">For a transformers model training, which is slower, the forward or the backward pass?</span>                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - meta-llama/Llama-3.3-70B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcomputational complexity of forward and backward passes in transformers models\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"computational complexity of forward and backward passes in transformers models\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">       </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(info)</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
              "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
              "backward\n",
              "\n",
              "===== Document 1 =====\n",
              "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
              "in \n",
              "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
              "needed \n",
              "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
              "\n",
              "===== Document 2 =====\n",
              "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
              "capacity. \n",
              "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
              "accumulation can \n",
              "slow down the training process.\n",
              "\n",
              "===== Document 3 =====\n",
              "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
              "translates \n",
              "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
              "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
              "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
              "forward,\n",
              "\n",
              "===== Document 4 =====\n",
              "The **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them \n",
              "for the \n",
              "entire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing \n",
              "forward \n",
              "and backward passes through the model and accumulating the gradients during the process. Once a sufficient number \n",
              "of \n",
              "gradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\n",
              "\n",
              "===== Document 5 =====\n",
              "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
              "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
              "especially for deployment on resource-constrained mobile devices\n",
              "\n",
              "===== Document 6 =====\n",
              "`N*K` forward passes through the model are required. This requirement slows inference considerably, particularly as\n",
              "`K` grows.\n",
              "\n",
              "===== Document 7 =====\n",
              "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
              "and returns the updated parameters.\n",
              "\n",
              "===== Document 8 =====\n",
              "## How to benchmark 🤗 Transformers models\n",
              "\n",
              "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
              "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
              "_training_.\n",
              "\n",
              "<Tip>\n",
              "\n",
              "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
              "backward pass.\n",
              "\n",
              "</Tip>\n",
              "\n",
              "===== Document 9 =====\n",
              ". We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and \n",
              "computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse \n",
              "models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off \n",
              "T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources\n",
              "\n",
              "Out: None\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "\n",
              "Retrieved documents:\n",
              "\n",
              "\n",
              "===== Document 0 =====\n",
              "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
              "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
              "backward\n",
              "\n",
              "===== Document 1 =====\n",
              "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
              "in \n",
              "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
              "needed \n",
              "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
              "\n",
              "===== Document 2 =====\n",
              "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
              "capacity. \n",
              "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
              "accumulation can \n",
              "slow down the training process.\n",
              "\n",
              "===== Document 3 =====\n",
              "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
              "translates \n",
              "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
              "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
              "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
              "forward,\n",
              "\n",
              "===== Document 4 =====\n",
              "The **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them \n",
              "for the \n",
              "entire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing \n",
              "forward \n",
              "and backward passes through the model and accumulating the gradients during the process. Once a sufficient number \n",
              "of \n",
              "gradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\n",
              "\n",
              "===== Document 5 =====\n",
              "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
              "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
              "especially for deployment on resource-constrained mobile devices\n",
              "\n",
              "===== Document 6 =====\n",
              "`N*K` forward passes through the model are required. This requirement slows inference considerably, particularly as\n",
              "`K` grows.\n",
              "\n",
              "===== Document 7 =====\n",
              "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
              "and returns the updated parameters.\n",
              "\n",
              "===== Document 8 =====\n",
              "## How to benchmark 🤗 Transformers models\n",
              "\n",
              "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
              "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
              "_training_.\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
              "backward pass.\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "===== Document 9 =====\n",
              ". We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and \n",
              "computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse \n",
              "models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off \n",
              "T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 2.08 seconds| Input tokens: 2,001 | Output tokens: 110]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.08 seconds| Input tokens: 2,001 | Output tokens: 110]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbackward pass\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"backward pass\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: backward pass\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: backward pass</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 2: Duration 1.64 seconds| Input tokens: 4,880 | Output tokens: 217]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 1.64 seconds| Input tokens: 4,880 | Output tokens: 217]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output:\n",
            "backward pass\n"
          ]
        }
      ],
      "source": [
        "agent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n",
        "\n",
        "print(\"Final output:\")\n",
        "print(agent_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}